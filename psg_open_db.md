# 오디오 신호 기반 수면 단계 추정 및 수면 호흡 장애 탐지를 위한 오픈소스 데이터셋 및 인공지능 모델 심층 분석 보고서

## 서론: 수면 음향 분석의 생리학적 기전 및 비침습적 모니터링으로의 패러다임 전환

수면 단계의 정확한 추정과 폐쇄성 수면 무호흡증(Obstructive Sleep Apnea, OSA)을 비롯한 수면 호흡 장애의 진단은 현대 예방 의학과 신경과학 분야에서 가장 시급하게 해결해야 할 중대한 과제로 자리 잡고 있다. 전통적으로 수면 질환 진단의 황금 표준(Gold Standard)은 뇌파(EEG), 안전도(EOG), 근전도(EMG), 심전도(ECG), 산소포화도(SpO2) 및 호흡 기류 센서 등 다양한 생리적 신호를 종합적으로 측정하는 수면다원검사(Polysomnography, PSG)이다. 그러나 PSG는 철저하게 통제된 병원 내 수면 검사실(In-lab) 환경에서 다수의 접촉식 유선 센서를 환자의 전신과 안면에 부착한 상태로 하룻밤을 보내야 하므로, 환자에게 심각한 신체적 및 심리적 불편함을 초래한다. 이러한 환경적 제약은 환자의 자연스러운 수면 패턴을 방해하고 비정상적인 각성을 유발하는 이른바 '초기 수면 효과(First-night effect)'를 야기하여 검사 결과의 생태학적 타당성을 저해할 수 있다. 또한, 검사 인프라 구축 비용이 매우 높고 수집된 방대한 생리 신호를 해석하기 위해 수면 전문의나 임상 병리사의 헌신적인 수동 스코어링(Manual scoring) 노동력이 필수적으로 요구되므로, 대규모 인구 집단에 대한 장기적이고 일상적인 연속 모니터링 도구로 활용하기에는 근본적인 한계가 존재한다.

이러한 물리적, 경제적 한계를 극복하기 위해 최근 인공지능(AI)과 디지털 바이오마커(Digital Biomarker) 추출 기술의 폭발적인 발전은 비침습적 비접촉식 센싱, 특히 스마트폰이나 웨어러블 기기에 내장된 마이크로폰을 통해 주변 환경의 오디오 신호를 포착하여 수면 단계 및 호흡 이벤트를 디코딩하는 방향으로 패러다임을 급격히 전환시키고 있다. 수면 중 발생하는 호흡음, 코골이 소리, 그리고 뒤척임에 의한 마찰음은 단순히 수면을 방해하는 무작위적인 환경 소음이 아니라, 상기도(Upper airway)의 생체역학적 상태와 중추 신경계의 자율 신경 제어 메커니즘을 실시간으로 반영하는 고도로 구조화된 임상적 생리 신호이다.

생리학적 관점에서 볼 때, 인간의 수면은 각성(Wake) 상태에서 시작하여 비렘수면(NREM Stage 1, 2, 3)을 거쳐 렘수면(REM)으로 진입하는 역동적인 주기를 밤새 반복한다. 이 수면 단계의 전이 과정에 따라 골격근의 긴장도, 자율 신경계의 교감 및 부교감 신경 활성도, 그리고 호흡의 규칙성과 깊이가 뚜렷하게 변화한다. 예를 들어, 얕은 비렘수면(NREM 1, 2)에서 깊은 서파 수면(NREM 3)으로 진입할수록 호흡은 더욱 느려지고 규칙적으로 변하며, 근육의 이완으로 인해 상기도 저항이 증가하여 호흡음의 저주파 대역 에너지가 상승하는 경향을 보인다. 반면, 렘수면(REM) 단계에서는 전신의 근육 무긴장증(Muscle atonia)이 발생하여 혀와 연구개가 기도를 압박할 확률이 극도로 높아지며, 호흡 패턴 자체가 매우 불규칙해지고 얕아져 특유의 난류성 호흡음이나 뚜렷한 코골이, 심지어는 호흡이 일시적으로 완전히 멈추는 무호흡 이벤트가 빈발하게 된다. 이러한 생리적 변화는 기도를 통과하는 공기의 역학적 마찰 계수와 조직의 진동 패턴을 변조하여, 음향학적 특성 즉 주파수 스펙트럼의 분포, 진폭 포락선(Amplitude envelope), 포먼트(Formant)와 같은 공명 특성의 점진적인 변화를 야기한다.

따라서 대규모 데이터로 정교하게 학습된 기계학습 및 딥러닝 아키텍처를 활용하면, 다채널 오디오 신호 내에 내재된 이러한 미세한 생리적 주기를 성공적으로 디코딩하여 고가의 PSG 장비와 유사한 수준의 신뢰도로 수면 단계 비율을 추정하고 치명적인 무호흡 이벤트를 식별하는 자동화 시스템을 구축할 수 있다. 본 심층 연구 보고서는 스마트폰 또는 전용 엣지 디바이스 기반의 오디오를 이용한 수면 단계 추정 및 수면 호흡 장애 탐지 시스템을 직접 개발하고자 하는 목적에 부합하도록 기획되었다. 이를 위해 현재 접근 가능한 동기화된 PSG-오디오 데이터가 포함된 오픈소스 데이터셋의 구조적, 기술적 특성을 극도로 상세하게 해부하고, 이를 기반으로 훈련되어 소스코드가 공개된 최신 인공지능 오픈 모델과 아키텍처의 방법론적 우수성, 기술적 난제, 그리고 실무적 구현 전략을 포괄적이고 입체적으로 고찰한다.

## 동기화된 PSG-오디오 오픈소스 데이터셋의 구조적 특성 및 활용 가능성 심층 분석

정확하고 강건하며 임상적으로 유효한 음향 기반 수면 추정 알고리즘을 개발하기 위한 가장 핵심적이고 절대적인 전제 조건은 고해상도 오디오 신호와 이에 완벽하게 동기화된 임상 표준 PSG 라벨(AASM 가이드라인 기준)이 쌍(Pair)을 이루어 결합된 대규모 데이터셋의 확보다. 과거 수십 년간 수면 의학 분야에서 음향 데이터와 PSG 데이터는 기술적, 행정적 한계로 인해 완전히 독립적으로 수집되거나 극히 제한적인 특정 병원의 코호트에서만 프라이버시 문제로 철저히 비공개로 관리되었다. 이로 인해 전 세계 연구자들은 알고리즘의 벤치마킹이나 재현성을 검증하는 데 극심한 어려움을 겪어왔다. 그러나 최근 Science Data Bank와 물리 기반 생체 신호 리포지토리인 PhysioNet 등을 통해 전례 없는 고해상도 다채널 오픈 데이터셋이 공개되기 시작하면서, 딥러닝 기반 음향 수면 연구는 거대한 도약의 전기를 맞이하고 있다. 본 장에서는 개발자가 즉시 활용할 수 있는 데이터셋과 그렇지 못한 데이터셋의 명확한 경계와 이유를 분석한다.

### 1. ScienceDB PSG-Audio 데이터셋: 임상 등급 오디오-PSG 동기화의 절대적 표준

현재 전 세계 학계 및 산업계에서 오디오 기반 수면 단계 추정과 호흡 이벤트 분할 모델링을 위해 가장 포괄적이고 광범위하게 참조되는 형태의 동기화 데이터셋은 그리스 아테네에 위치한 Sismanoglio - Amalia Fleming General Hospital의 임상 의료진과 외부 연구 기관들이 공동으로 구축하여 Science Data Bank 플랫폼에 영구적으로 공개한 `PSG-Audio` 데이터셋이다. 이 데이터셋은 CC BY 4.0(저작자표시 4.0 국제) 라이선스 하에 비상업적, 상업적 연구 목적을 막론하고 완전히 개방되어 있으며(공식 DOI: 10.11922/sciencedb.00345), 수면 연구 분야에 전례 없는 기여를 하고 있다.

이 코호트는 수면 다원 검사 의뢰를 받은 총 212명의 성인 환자(남성 156명, 여성 56명, 연령 분포 34세~76세)로부터 통제된 수면 검사실에서 수집된 전야(Whole-night) 수면 기록을 온전히 포함하고 있다. 이 데이터셋이 지니는 가장 독보적인 학술적, 공학적 가치는 임상 등급의 다채널 생리적 신호(EDF 포맷)와 상이한 목적을 지닌 두 가지 별개의 음향 센서 네트워크가 밀리초(ms) 단위의 오차 없이 완벽하게 시간 동기화(Time-synchronized)되어 있다는 점이다. 오디오 신호 수집을 위해 전문가용 휴대용 2채널 멀티트랙 레코더인 Tascam DR-680 MK II 장비가 동원되었으며, 손실 압축 알고리즘이 전혀 적용되지 않은 24비트 심도(Bit depth)의 비압축 WAV 포맷으로 수집되었다. 특히 48kHz라는 스튜디오 녹음 수준의 초고해상도 샘플링 속도(Sampling rate)를 채택함으로써, 저주파 대역의 무거운 코골이 진동음부터 기류 제한 시 발생하는 고주파 대역의 미세한 천명음(Wheezing)이나 마찰음의 나이퀴스트 주파수(Nyquist frequency) 한계를 완전히 극복하여, 어떠한 주파수 성분의 손실 없이 신호를 보존하고 있다.

아래의 표는 PSG-Audio 데이터셋에 포함된 다중 모달리티 센서들의 정밀한 하드웨어 사양과 신호 특성을 요약한 것이다.

|**채널 라벨 (Channel Label)**|**센서 사양, 배치 및 생리적 역할 설명 (Description)**|**샘플링 속도 (Hz)**|**디지털 신호 해상도 (Max Range)**|
|---|---|---|---|
|`Tracheal`|접촉식 일렉트릿 마이크(Clockaudio CTH100). 환자의 목 부위 기관(Trachea)에 직접 밀착 부착. 900 Ω의 높은 입력 임피던스와 350 Hz – 8 kHz의 통과 대역(Passband)을 지녀 주변 환경 소음을 물리적으로 차단하고 순수한 성도 진동만을 수집함.|48,000|32,767 (16-bit PCM equivalent)|
|`Microphone`|주변 공간 측정용 콘덴서 마이크(Behringer ECM8000). 환자 머리 위 약 1미터 지점 천장에 배치. 전방향성(Omnidirectional) 폴라 패턴, 200 Ω 임피던스, 15 Hz–20 kHz의 극도로 평탄한 주파수 응답(Flat frequency response)을 통해 가정 환경과 유사한 실내 잔향과 기류 소음을 포착함.|48,000|32,767 (16-bit PCM equivalent)|
|`SpO2`|산소포화도 측정기(Oxymeter). 무호흡 이벤트 직후 발생하는 혈중 산소 농도의 지연된 강하(Desaturation) 패턴을 추적하는 핵심 지표.|1|1,023|
|`Body`|신체 수면 자세 지표. 바로 누운 자세(Supine, S), 우측와위(Right, R), 좌측와위(Left, L)를 구별하여 중력에 의한 기도 폐쇄 영향을 보정함.|1|255|
|`PulseRate`|심전도(ECG) 1유도에서 추출된 맥박수. 자율 신경계 활성도 변화를 대변.|1|255|
|`Effort ABD`|복부 호흡 벨트. 흉곽과 복부의 체적 변화를 통해 중추성 무호흡(호흡 노력 없음)과 폐쇄성 무호흡(호흡 노력 있음)을 판별하는 기준.|100|32,767|

표 1: PSG-Audio 데이터셋에 포함된 핵심 음향 및 생리 센서들의 기술적 명세 (자료 출처: 데이터셋 메타데이터 및 관련 문헌 )

이러한 이중 마이크로폰 시스템(`Tracheal` 및 `Microphone`) 구성은 딥러닝 연구자들에게 상상할 수 없는 이점을 제공한다. 주변 환경 소음에 거의 영향을 받지 않고 오직 환자의 체내 진동만을 순수하게 녹음하는 접촉식 기관 마이크 신호는, 호흡과 무호흡의 정확한 온셋(Onset)과 오프셋(Offset)을 결정하는 강력한 그라운드 트루스(Ground truth) 보조 지표로 기능한다. 반면 머리 위 1미터에 배치된 주변 공간 마이크 신호는 향후 사용자가 스마트폰을 침대 협탁에 올려두고 수면을 취하는 실제 사용 사례(Real-world Use Case)를 가장 완벽하게 모사한다. 따라서 연구자는 접촉식 마이크 데이터를 교사 신호(Teacher signal)로 삼아 공간 마이크(Student signal) 기반 모델을 전이 학습(Transfer learning)시키거나, 두 도메인 간의 오차를 줄이는 센서 융합 모델링의 이상적인 샌드박스로 이 데이터셋을 활용할 수 있다.

또한, 데이터셋의 라벨링 품질 측면에서 이 데이터셋은 미국 수면의학회(AASM)의 2012년 엄격한 가이드라인 기준을 준수한다. 숙련된 전문의들에 의해 수면 단계(Wake, REM, NREM 1, 2, 3)가 30초 단위의 비오버랩(Non-overlapping) 에포크로 전수 수동 스코어링되었으며, 무호흡(Apnea) 및 저호흡(Hypopnea) 등 호흡기 이벤트의 발생 시점, 지속 시간, 하위 유형(폐쇄성, 중추성, 혼합성)이 기계가 즉각적으로 파싱할 수 있는 XML 기반의 RML(.rml) 파일 형식으로 제공된다. 이는 단순한 코골이 유무만을 분류하는 구형 데이터셋(예: Munich-Passau Snore Sound Corpus 등)이 무호흡의 존재 여부를 전혀 주석 처리하지 못한 치명적인 한계를 완벽히 극복한 것이다.

다만, 모델 설계 및 훈련 시 반드시 주의해야 할 통계적 편향성(Statistical Bias)이 존재한다. 데이터 수집을 목적으로 방문한 임상 코호트의 특성상 전체 212명의 참가자 중 무려 88.7%가 무호흡-저호흡 지수(AHI) 30 이상의 중증 폐쇄성 수면 무호흡증(Severe OSA) 환자로 구성되어 있다. 이는 모델이 이상 호흡음 패턴이나 극단적인 헐떡임(Gasping) 현상을 학습하는 데에는 매우 풍부한 긍정적 샘플을 제공하여 유리하지만, 정상 수면 패턴이나 건강한 대조군(Healthy control)의 비율이 턱없이 부족하여 정상 상태(Negative sample)에 대한 심각한 클래스 불균형(Class Imbalance) 문제를 태생적으로 내포하고 있음을 의미한다. 딥러닝 모델이 이러한 불균형을 보정하는 특별한 손실 함수 없이 학습될 경우, 모든 호흡을 잠재적 무호흡으로 과대 예측(Over-predict)하는 특이도(Specificity) 저하 문제에 직면할 위험이 매우 높다.

### 2. Kaggle 파생 데이터셋 및 전처리 최적화 파이프라인의 유용성

앞서 기술한 ScienceDB의 원본 `PSG-Audio` 데이터셋은 그 거대한 용량(환자당 수 GB에 달하는 WAV 파일들)과 48kHz의 높은 샘플링 레이트로 인해, 초기 알고리즘의 프로토타이핑이나 학부 및 석사 수준의 제한된 GPU 컴퓨팅 자원 하에서는 심각한 입출력(I/O) 병목 현상을 유발한다. 이러한 인프라적 병목을 해소하고 연구의 진입 장벽을 낮추기 위해, 오픈소스 커뮤니티의 기여자들은 원본 데이터를 정제한 파생 데이터셋을 배포하고 있다.

가장 대표적이고 유용한 예시가 Kaggle 플랫폼에 호스팅된 `PSG-Audio Apnea Audios` 데이터셋(Bryan Darquea 기여)이다. 이 데이터셋은 원본 EDF 및 WAV 파일에서 공간 마이크(`Mic`) 채널의 오디오 파형만을 선택적으로 추출한 후, 음성 인식 모델에 가장 널리 쓰이는 표준 샘플링 주파수인 16kHz로 다운샘플링(Downsampling) 처리를 완료한 상태로 제공된다. 비록 16kHz 다운샘플링 과정에서 8kHz 이상의 초고주파 대역 정보는 나이퀴스트 이론에 따라 소실되지만, 코골이나 무호흡 등 대부분의 인간 호흡 관련 에너지 스펙트럼은 2kHz 미만에 집중되어 있으므로 모델의 분류 성능에 미치는 타격은 미미한 반면, 텐서(Tensor) 연산량은 3분의 1 수준으로 급감한다. 또한 RML 파일의 텍스트 주석을 파싱하여 원시 오디오 시계열을 10초 단위의 고정된 이벤트 분절(Apnea, Near-apnea 이벤트 및 무작위 추출된 Normal sleep 이벤트)로 잘라내어, 파이썬(Python) 기계학습 생태계에서 즉각적으로 배열(Array) 형태로 로드할 수 있는 NPY(NumPy) 파일 포맷으로 완벽히 구조화해 두었다. 이 파생 데이터셋은 모델 훈련의 즉각적인 투입을 가능하게 하며, 데이터 로더(Data Loader) 구현에 소요되는 엔지니어링 비용을 극단적으로 단축하여 시스템 파이프라인의 초기 성능 검증 및 아키텍처 디버깅 단계에서 상상 이상의 효율성을 발휘한다.

### 3. 대규모 폐쇄형 및 비음향 코호트 데이터셋의 한계적 맥락 파악

현재 가용한 유일한 고해상도 오픈 오디오 데이터셋이 ScienceDB의 `PSG-Audio`라는 결론을 도출하기 위해서는, 수면 연구 분야에서 압도적인 위상을 차지하고 있는 다른 대규모 데이터베이스들이 본 질의(오디오를 이용한 시스템 개발)에 부합하지 않는 근본적인 이유를 교차 검증하는 과정이 필수적이다. 이는 연구자가 잘못된 데이터셋을 탐색하는 데 시간을 낭비하는 것을 방지하는 핵심 통찰이다.

첫째, 서울대학교병원(SNUH) 및 분당서울대학교병원(SNUBH)의 연구진들이 구축한 코호트 데이터셋이다. 이 데이터셋은 1,018건의 병원 내 임상 PSG-오디오 쌍과, 특히 본 시스템 개발의 궁극적 지향점인 스마트폰을 활용한 가정 내 수면 환경(Home Environment) 오디오 녹음 327건을 동기화하여 포함하고 있다는 점에서 기술적으로 가장 완벽한 코호트로 평가받는다. 병원 수면실 천장에 설치된 마이크(SUPR-102 칩)와 가정 침대 협탁 위의 스마트폰 마이크라는 이질적인 장비를 거친 데이터는, 실생활 배포 모델의 잡음 강인성을 테스트하는 데 최적화되어 있다. 그러나 치명적이게도 환자의 음성(Voice)이 원시 오디오 신호에 노출될 수 있어, 이를 멜 스펙트로그램(Mel-Spectrogram)으로 변환하더라도 역공학(Reverse-engineering)을 통해 개인의 목소리나 민감한 사적 대화 내용을 재구성할 수 있다는 프라이버시 침해(Privacy Risk) 우려와 임상시험심사위원회(IRB)의 엄격한 윤리적 규제로 인해, 어떠한 형태의 원시 데이터나 파생 특징(Feature)도 대중 및 연구자에게 절대 오픈소스로 공개되지 않는다.

둘째, 미국 국립심장폐혈액연구소(NHLBI)의 지원을 받아 National Sleep Research Resource (NSRR) 플랫폼(sleepdata.org)에서 호스팅되는 초대형 국가 단위 수면 코호트들이다. SHHS (Sleep Heart Health Study), MESA (Multi-Ethnic Study of Atherosclerosis), CFS, MrOS 등은 적게는 수백 명에서 많게는 8,000명 이상의 수면 데이터를 보유하며, 누적 데이터양이 2 페타바이트(Petabyte)에 달한다. 그러나 이들 데이터베이스는 수면 단계 추정을 위해 철저하게 뇌파(EEG), 심전도(ECG), 산소포화도, 그리고 코 밑에 장착하는 비강 캐뉼라(Nasal Cannula)의 압력 변환 기류 신호에만 의존한다. 비록 코골이 센서가 부착된 경우가 간혹 있으나, 이는 피에조 센서를 통한 단순한 진동 강도의 로그 기록일 뿐, 우리가 기계학습에 사용할 수 있는 광대역 주파수를 포함한 고해상도 원시 오디오 신호(Raw Audio WAV) 파일 포맷은 근본적으로 포함되어 있지 않거나 제공되지 않는다.

셋째, 생체 신호 데이터베이스의 성지라 불리는 PhysioNet의 데이터셋들이다. 매우 유명한 Sleep-EDF 및 Sleep-EDFx 데이터베이스는 197건의 전야 PSG를 제공하여 EEG 기반 수면 딥러닝 연구의 영원한 벤치마크로 군림하고 있으나, 마이크로폰 오디오 채널은 단 하나도 존재하지 않는다. 최근(2025년 공개) 큰 주목을 받은 DREAMT (Dataset for Real-time sleep stage EstimAtion using Multisensor wearable Technology) 데이터셋의 경우, 100명의 수면 호흡 이상 환자를 대상으로 고해상도 웨어러블 디바이스(Empatica E4 스마트워치) 신호와 임상 PSG 라벨을 동기화하여 제공한다. 4Hz~200Hz에 이르는 혈류용적맥파(BVP), 3축 가속도(ACC), 피부전도도(EDA) 등을 제공하여 웨어러블 기반 수면 단계 추정에는 가뭄의 단비 같은 존재이나, 역시 본 질의의 핵심인 마이크로폰 오디오 채널은 포함하지 않는다. 동일하게 3,984건의 소아 수면 데이터를 보유한 NCH Sleep DataBank 역시 광범위한 임상 주석과 전자 의무 기록(EHR)을 제공할 뿐 음성 데이터를 취급하지 않는다.

결론적으로, 현재 시점에서 스마트폰이나 음성 스피커 등을 타겟으로 한 오디오 기반 수면 단계 추정 및 호흡 장애 시스템 개발에 즉시 다운로드하여 모델을 훈련시킬 수 있는 실질적이고 유일하며 가장 품질이 뛰어난 고해상도 오픈 데이터는 ScienceDB의 `PSG-Audio` 데이터셋으로 귀결된다.

## 음향 기반 수면 단계 및 이벤트 탐지를 위한 딥러닝 아키텍처 및 오픈소스 모델

오디오를 매개로 인간의 수면 상태와 비정상적인 호흡 이벤트를 디코딩하는 작업은 기계학습의 관점에서 볼 때 본질적으로 매우 복잡한 다변량 시계열 패턴 인식(Multivariate Time-series Pattern Recognition) 문제로 환원된다. 초창기의 전통적인 분석 방법론은 멜 주파수 셉스트럼 계수(MFCC)나 단순한 에너지 포락선(Energy envelope), 영점 교차율(Zero-crossing rate)과 같은 인간이 사전 정의한 수작업 특징(Hand-crafted features)을 추출한 뒤, 서포트 벡터 머신(SVM)이나 랜덤 포레스트(Random Forest)를 적용하여 이진 분류를 수행하는 얕은(Shallow) 수준에 머물렀다. 하지만 최근 공개된 학계의 오픈소스 모델들은 이러한 낡은 패러다임을 완전히 탈피하였다. 오디오 스펙트로그램의 시공간적 역학을 데이터로부터 스스로 학습하는 심층 합성곱 신경망(CNN)과 장기 의존성을 포착하는 장단기 메모리(LSTM) 네트워크의 결합을 넘어서서, 최근에는 수백만 시간의 음성 데이터로 거대 자기 지도 학습(Self-Supervised Learning, SSL)을 거친 트랜스포머(Transformer) 파운데이션 모델을 수면 음향 분석의 백본(Backbone)으로 직접 이식하는 차세대 구조로 진화하였다. 본 장에서는 개발자가 GitHub 등에서 오픈소스로 확보하여 즉각 참조할 수 있는 최첨단 아키텍처와 그 작동 원리를 심층 분석한다.

### 1. 기반 기술의 진화: 거대 음성 파운데이션 모델 (Speech Foundation Models)의 생리학적 전이

음성 신호는 단순히 단어와 문장이라는 언어적 의미(Linguistic content)만을 전달하는 것이 아니라, 성도(Vocal tract)의 물리적 3차원 구조, 폐활량과 횡격막의 움직임, 호흡 기관 내부의 압력 구배(Pressure gradient), 그리고 수면 중 조직의 탄성 등 상상 이상으로 풍부한 생리학적 메타데이터를 캐리어 파형(Carrier wave) 내에 암호화하여 내포하고 있다. 따라서 대규모 인류 음성 코퍼스(Corpus)에서 막대한 컴퓨팅 파워로 사전 훈련된 거대 파운데이션 모델(Foundation Models)인 OpenAI의 Whisper, Facebook의 HuBERT (Hidden-Unit BERT), WavLM 등은 비록 그들이 언어 인식을 목적으로 탄생했을지라도, 인간의 발성 기관을 거쳐 나오는 수면 중 호흡음과 코골이 소리의 잠재적 특징(Latent representations)을 고차원적으로 추출하고 분류하는 데 탁월한 범용적 성능을 발휘한다.

2025년 IEEE EMBC (Engineering in Medicine and Biology Society) 등 최신 학술 대회에 발표된 소스코드 공개 문헌에 따르면, HuBERT 기반의 사전 학습 인코더를 적용한 수면 단계 추정 모델은 데이터 전처리 과정에서 필수적이었던 명시적인 스펙트로그램 변환이나 주파수 필터링 과정조차 생략한 채, 오디오 1차원 파형(Raw Waveform) 자체를 직접 입력받아 호흡률의 미세한 변동과 코골이 주파수의 시프트 등 수면 단계를 결정짓는 생리학적 바이오마커를 효과적으로 포착해 낸다. 연구진들은 전통적인 30초 단위의 단일 에포크 분석이 지닌 시간적 시야의 한계를 극복하기 위해, 인접한 시간적 맥락을 통합하는 90초 길이의 이동 창(Moving window)에 걸쳐 HuBERT의 잠재 임베딩(Latent embeddings)을 집계(Aggregation)하는 구조를 제안하였고, 이를 통해 특히 얕은 수면(Light Sleep)과 깊은 수면(Deep Sleep) 등 NREM 수면 내 세부 단계의 분류 민감도를 비약적으로 끌어올리는 성과를 달성하였다. 이러한 거대 파운데이션 모델을 활용한 연구의 소스코드는 GitHub 저장소(예: gultahaoglu/Deepfake-audio-detection-SSLFeatures-NextTDNN 아키텍처의 변형 등)를 통해 파인튜닝 스크립트 형태로 연구자들에게 공유되고 있으며, 이는 수면 음향 데이터가 부족한 상황에서도 모델이 과적합(Overfitting)되지 않고 도메인 일반화 능력(Domain Generalization)을 극대화하도록 돕는 중추적인 베이스라인이 된다.

### 2. ApneaWhisper: 고해상도 프레임 레벨 미세 분할의 혁신적 아키텍처

만약 사용자의 개발 목표가 단순히 수면 단계를 3~4개로 분류하는 것을 넘어, 구체적인 수면 무호흡증 이벤트를 탐지하고 임상적으로 유의미한 지수인 무호흡-저호흡 지수(AHI)를 오디오만으로 계산하는 것이라면, 현재 학계에 공개된 가장 진보된 오픈 모델 구조인 `ApneaWhisper`를 집중적으로 벤치마킹해야 한다. 2025년 Nature and Science of Sleep 저널 등에 발표된 최신 연구인 이 모델은 ScienceDB의 PSG-Audio 데이터셋을 활용하여 수면 호흡 장애의 하위 유형(Subtypes: 폐쇄성 OSA, 중추성 CSA, 혼합성 MSA, 저호흡 Hypopnea)을 비침습적으로 세밀하게 탐지하고 분할(Fine-Grained Segmentation)하기 위해 고안된 트랜스포머 기반의 아키텍처다.

기존의 딥러닝 연구들이 주로 30초 또는 1분 길이의 오디오 구간 전체를 입력으로 받아 단순히 해당 구간 내에 "무호흡 이벤트가 존재하는가(Binary classification)"만을 판별하는 거시적 분류에 그쳤다면, ApneaWhisper는 대규모 음성 인식 모델인 Whisper 인코더의 파워를 활용하여 시간 축에 대한 극도의 해상도를 확보하는 구조적 혁신을 이루어냈다.

분석 메커니즘을 층위별로 해부해 보면 다음과 같다. 먼저 시스템에 입력된 20초 길이의 원시 오디오 클립은 사전 학습된(Pretrained) Whisper 인코더를 통과하면서 10밀리초(10ms, 즉 1초당 100프레임) 단위의 엄청난 고해상도 시계열 프레임 수준 특징(Frame-level features) 벡터 시퀀스로 투영(Projection)되어 추출된다. 추출된 이 초정밀 특징 벡터들은 거대한 디코더 대신 모델 파라미터가 최적화된 경량화된 트랜스포머 디코더(Lightweight Transformer Decoder) 네트워크로 전달된다. 이 디코더는 토큰 기반(Token-based)의 어텐션 분할을 수행하여 각 10ms 프레임이 어느 호흡 상태에 속하는지를 확률적으로 계산하며, 마지막 계층에 배치된 분류 헤드(Classification head)를 통해 프레임 단위의 시작과 끝 예측(Frame-level prediction)과 20초 클립 단위의 거시적 이벤트 분류(Clip-level prediction)를 이중으로 동시에 수행하는 멀티태스크 구조를 갖는다.

이러한 프레임 분할(Segmentation) 모델 구조의 도입이 지니는 핵심적인 임상적 돌파구는, 이벤트의 단순한 존재 유무를 넘어 정확히 "무호흡이 언제(몇 초 몇 밀리초에) 시작해서 언제 종료되었는가"를 정밀하게 추적함으로써 정확한 이벤트 지속 시간(Duration)을 측정해 낸다는 것이다. 이를 통해 병원의 복잡한 다채널 PSG 장비 없이 스마트폰 마이크 하나만으로도 환자의 전체 수면 시간 대비 정확한 무호흡-저호흡 지수(AHI)를 산출해 내는 것이 가능해졌다. 독립적인 검증 실험 결과, ApneaWhisper 구조는 무호흡 탐지에서 클립 레벨 F1-스코어 0.82, 프레임 레벨 F1-스코어 0.70이라는 뛰어난 성능 지표를 달성하여, 기존 산업계에서 널리 쓰이던 MFCC+DNN 베이스라인이나 구글의 VGGish 오디오 추출기 및 bi-LSTM을 혼합한 전통적 아키텍처의 성능을 큰 폭으로 상회하는 압도적 우수성을 입증하였다.

이 모델과 관련된 파이프라인 개념 증명과 사전 학습된 웨이트(Pretrained weights)는 학술 커뮤니티 내 연관 연구자들의 GitHub 저장소(예: LilloByte/SiCRNN 연계 리포지토리 또는 AmbitYuki/HeteroGCFNet 시계열 분석 구조)들을 통해 파편적으로나마 오픈소스 생태계에 편입되고 있어, 자체적인 시스템 아키텍처를 스케치할 때 이 구조를 레퍼런스로 삼는 것은 가장 확실한 성공 전략이다. 다만 논문에서도 한계로 지적되었듯, 폐쇄성 무호흡(OSA)에 대해서는 진동 패턴이 명확하여 매우 강건한 탐지력을 보이나, 호흡 노력 자체가 사라지는 중추성 무호흡(CSA)이나 혼합성 무호흡(MSA)의 경우 음향적 패턴(Acoustic signature)이 극도로 미묘하여 분류의 성공률이 변동한다는 점은 향후 모델 개선 시 산소포화도(SpO2) 등 모달리티 추가 융합이 필요한 지점이다.

### 3. SoundSleepNet 및 Audio Spectrogram Transformer (AST) 기반 실무 구현체

시계열 파형 기반의 트랜스포머 접근과 양대 산맥을 이루는 또 다른 주류 아키텍처는 `SoundSleepNet` 연구 계보를 잇는 스펙트로그램-시퀀스 시공간 학습(Spatio-temporal spectrogram learning) 파이프라인이다. 이 시스템들은 원시 오디오 파형이 지닌 고주파 노이즈의 민감도를 낮추기 위해, 신호를 고속 푸리에 변환(FFT)하여 인간의 청각 인지 모델을 반영한 멜 스펙트로그램(Mel-Spectrogram) 2D 이미지 패치로 변환한 후 컴퓨터 비전(Computer Vision) 기술을 접목하여 시공간적 특성으로 다룬다.

SoundSleepNet의 기본 설계 철학은 철저한 계층적 특징 추출에 있다. 네트워크의 하단에서 'Listener(청취자)' 역할을 수행하는 다중 계층의 합성곱 신경망(CNN)이나 잔차 네트워크(ResNet)가 단일 30초 에포크의 멜 스펙트로그램 이미지 내에서 발생하는 짧은 호흡 마찰음과 국소적인 주파수 패턴을 1차원적인 고밀도 특징 벡터로 압축 추출한다. 이후 이렇게 압축된 피처 시퀀스가 상단에 쌓인 다층 양방향 장단기 메모리(Bidirectional LSTM) 네트워크나 트랜스포머 인코더로 전달되어, 밤새도록 길게 이어지는 수면 주기의 변천(Sleep architecture progression)과 같은 장기적인 시간적 의존성(Long-term temporal correlation)을 완벽하게 모델링하는 이중 구조를 띤다.

이러한 설계 철학을 가장 완벽하게 실무적인 코드로 구현하여 오픈소스로 공개한 대표적인 사례가 GitHub에 등록된 `Sleep-Stage-Classification` 저장소(AllyHyeseongKim 등 구현)이다. 이 저장소는 학계의 이론을 실제 애플리케이션 개발 수준으로 끌어올려 놓았다. 구체적으로 Google이 수백만 개의 YouTube 오디오 데이터로 사전 학습시켜 공개한 강력한 오디오 이벤트 분류기인 YAMNET과 최신 Audio Spectrogram Transformer (AST) 구조를 채택하여 수면 사운드를 지능적으로 분류하는 프레임워크를 파이썬(Python)과 TensorFlow 환경에서 통합적으로 제공한다. YAMNET 백본 네트워크를 전처리 피처 추출기로 활용함으로써 일상적이고 다양한 홈 환경 배경 소음 속에서도 수면과 관련된 핵심 생리음만을 1차적으로 강건하게 필터링해 낸다. 더욱 중요한 점은, 이렇게 학습된 무거운 모델을 모바일이나 웨어러블 같은 엣지 디바이스 환경(On-device)에서 오프라인 실시간 추론이 가능하도록 구글의 경량화 프레임워크인 TFLite 변환 기능까지 코드로 제공하여, 클라우드 서버에 민감한 오디오 데이터를 전송할 필요 없이 로컬에서 분석을 완결 짓도록 설계되어 있다는 점이다. 해당 리포지토리는 데이터 전처리를 담당하는 로더(`data.py`), 딥러닝 모델 아키텍처 정의(`models.py`), 모델의 훈련 및 평가 로직(`main.py`), 그리고 엣지 변환기(`tflite_converter.py`) 모듈을 모두 독립적이고 모듈화된 형태로 포괄하고 있어, 앞서 분석한 ScienceDB의 PSG-Audio 커스텀 데이터셋을 로더에 연결하기만 하면 곧바로 상용화 수준의 모델 훈련과 튜닝을 시작할 수 있는 강력하고 유연한 엔지니어링 템플릿으로 평가된다.

또한, 원래는 단일 채널 뇌파(EEG) 기반의 수면 단계 추정에 특화되어 개발되었으나 구조적 우수성 덕분에 멀티모달 프레임워크로 각광받는 `AttnSleep` (GitHub: emadeldeen24/AttnSleep) 모델의 소스코드 역시 주목해야 한다. 이 모델은 시계열 데이터를 1D-CNN을 통해 로컬 피처를 추출하고, Multi-head Self-Attention 구조를 통해 전역적인 시퀀스 맥락을 병렬로 해석하는 구조를 띠고 있어, 약간의 입력 텐서 차원 변형(Adaptation)만 가하면 오디오 시계열 수면 단계 분류기로 훌륭하게 전환 응용될 수 있으며, 실제로 다양한 파생 논문에서 높은 성능의 베이스라인 재현 검증용 코드로 널리 사용되고 있다.

### 4. U-Sleep 기반의 시계열 완전 합성곱 네트워크의 거시적 접근

마지막으로 검토해야 할 선도적 아키텍처는 다채널 생리 신호 분석을 위해 고안된 시계열 완전 합성곱 네트워크(Fully Convolutional Network)인 `U-Sleep`의 파라다임이다. 본래 수만 시간의 다기관 PSG 데이터를 통합 학습하기 위해 설계된 이 모델은 컴퓨터 비전에서 이미지 세그멘테이션의 혁명을 일으킨 U-Net 아키텍처를 1차원 시계열 데이터에 맞게 변형한 구조다. U-Sleep 아키텍처는 개별 30초 에포크를 독립적으로 잘라서 처리하는 시퀀스-투-시퀀스(Seq2Seq) 방식을 과감히 버리고, 하룻밤 전체의 완전한 폴리솜노그램(Whole-night recording) 원시 파형 시퀀스를 모델의 입력으로 한꺼번에 통과시킨다.

모델의 인코더 계층은 시계열을 지속적으로 압축하며 멀티 채널 간의 상관관계와 호흡 이벤트의 고수준 추상적 특징을 추출하고, 디코더 계층은 이를 다시 원래의 시간 해상도로 확장(Upsampling)하면서 스킵 커넥션(Skip connection)을 통해 미세한 시간적 경계선의 정보를 복원하여 최종적으로 각 시점의 단일 태그(수면 단계 또는 이벤트 유무) 시퀀스를 출력한다. 이 방식은 앞서 언급한 장기 의존성(Long-term dependencies)을 모델 구조 자체가 선천적으로 학습하도록 강제하여, 모델이 예측 과정에서 일시적인 찰나의 노이즈나 아티팩트에 속아 수면 단계를 널뛰듯 잘못 예측하는 플리커링 오류를 방지하는 고주파 복원력(Resilient high-frequency sleep staging)을 제공한다. 오디오 데이터베이스를 다룰 때 U-Sleep의 구조적 모티프를 차용하여 오디오 특징 벡터의 전체 야간 시퀀스를 인코더-디코더 맵핑으로 처리한다면, 환자의 시간대별 호흡 패턴 변화를 거시적인 흐름 속에서 추적하는 대단히 견고한 시스템을 구축할 수 있다.

|**아키텍처 모델명**|**핵심 구조 및 백본 (Backbone)**|**주요 특징 및 시간 해상도**|**오픈소스 가용성 및 주요 활용 저장소**|
|---|---|---|---|
|**ApneaWhisper**|OpenAI Whisper 인코더 + 트랜스포머 디코더 기반|10밀리초(ms) 단위의 초정밀 프레임 레벨 분할, 이벤트 시작/종료점 추적 및 AHI 직접 산출|논문 부록 및 파생 구현체 (GitHub 참조 가능: LilloByte/SiCRNN 연계 구조 등)|
|**SoundSleepNet / AST**|2D Mel-Spectrogram + YAMNET + AST / 다층 양방향 LSTM|30초 에포크 및 인접 컨텍스트 학습. 환경 노이즈 필터링 우수 및 TFLite 엣지 배포 용이|완전 개방형 소스코드 (GitHub: `Sleep-Stage-Classification` 프로젝트 등)|
|**AttnSleep 구조 응용**|1D-CNN + Multi-head Self-Attention 하이브리드|1차원 시계열 병렬 처리 특화. 연산 효율성이 높고 시퀀스 내 중요 이벤트에 어텐션 집중|완전 개방형 소스코드 (GitHub: `emadeldeen24/AttnSleep`)|
|**U-Sleep 패러다임**|시계열 1D U-Net 기반 완전 합성곱 네트워크|전야(Whole-night) 시퀀스 일괄 입력 및 출력 매핑. 수면 전이 구간의 플리커링 오류 최소화|논문 기반의 아키텍처 공개 (모델 구현 코드 자체는 파이토치 등으로 재현 용이)|

_표 2: 음향 기반 수면 단계 및 이벤트 탐지를 주도하는 핵심 딥러닝 아키텍처 및 특성 비교 요약_

## 현장 배포 및 시스템 구현을 위한 기술적 난제와 알고리즘적 해결 전략

앞서 분석한 극도로 우수한 구조의 오픈 데이터셋과 최첨단 인공지능 오픈 모델 코드를 성공적으로 통합하여, 단순한 연구실용 데모를 넘어 실제 사용자 환경에서 완벽하게 동작하는 상용 수준의 추정 시스템을 개발하기 위해서는 현장에서 필연적으로 발생하는 세 가지 핵심적인 데이터 과학적 장벽을 극복해야 한다. 모델 개발자는 아래의 해결 방법론들을 신경망 설계 단계에서부터 알고리즘 내에 구조적으로 내재화해야만 높은 정확도와 강건성을 담보할 수 있다.

### 1. 극단적 클래스 불균형 (Extreme Class Imbalance)과 비대칭적 수면 구조의 통계적 보정

인간의 정상적인 하룻밤 수면 구조에서 가장 얕은 비렘수면인 NREM 2단계(Light Sleep)는 전체 수면 시간의 약 50% 이상이라는 압도적인 비중을 차지하는 반면, 렘수면(REM)이나 깊은 서파 수면(NREM 3), 그리고 질환적 측면에서 특정 무호흡 하위 이벤트(예: 중추성 수면 무호흡증, CSA)는 전체 시간 대비 발생하는 빈도가 지극히 낮다. 기계학습에 사용할 오디오 데이터 관점에서 이러한 자연적 현상은 치명적인 제약으로 다가오는데, 아무 조치 없이 표준 훈련을 진행할 경우 딥러닝 모델은 손실(Loss)을 최소화하려는 수학적 본능에 따라 단순히 모든 입력 에포크를 확률이 가장 높은 '다수 클래스(Majority class, 예: NREM 2 또는 정상 호흡)'로만 편향되게 예측해버리는 치명적 오류(Over-predicting dominant classes)를 범하게 된다. 이는 모델의 겉보기 전체 정확도(Accuracy) 지표는 80% 이상으로 높게 포장되게 만들지만, 실제 우리가 가장 찾아내야 할 중요 이벤트인 무호흡이나 렘수면에 대한 민감도(Sensitivity)는 처참하게 훼손되는 결과를 낳는다.

이에 대한 통계적 해법으로 `ApneaWhisper`와 같은 최신 프레임워크는 비용 함수를 근본적으로 뜯어고쳐, 표준 교차 엔트로피(Standard Cross-entropy) 대신 '클래스 균형 교차 엔트로피 손실 함수(Class-Balanced Cross-Entropy Loss)' 또는 유사한 형태의 포컬 로스(Focal Loss)를 신경망 훈련망에 선제적으로 도입하였다. 이 진보된 손실 함수는 학습 데이터 배치(Batch)가 들어올 때마다, 발현 빈도가 적어 데이터 희소성을 띠는 소수 클래스(Minority classes: 예: CSA, NREM 3)의 예측 오답에 대해서는 기하급수적으로 높은 페널티 가중치(Weight)를 동적으로 부여하고, 반대로 너무 흔하게 등장하는 다수 클래스(예: 정상 호흡)의 오답에는 상대적으로 낮은 가중치를 할당하여 기울기 업데이트(Gradient update)의 균형을 강제로 맞춘다.

이러한 최적화 기법을 도입하면, 인공지능 모델은 모호한 환경 소음이나 미묘한 음향 패턴 속에서도 숨겨진 희귀 생리 신호를 찾아내기 위한 세밀한 결정 경계(Decision boundary)를 강제적으로 세밀하게 분할 학습하게 되며, 결과적으로 심각하게 불균형한 PSG-Audio 데이터셋 하에서도 모든 수면 단계 및 질환 하위 이벤트에 대해 견고하고 편향 없는 표상(Robust representation)을 획득하는 데 성공하게 된다. 시스템 개발 시 오픈 모델 소스코드를 융합할 때, 데이터 로더(`data.py`)의 전처리 설계 단계에서부터 소수 클래스의 언더샘플링(Undersampling)이나 SMOTE 등 오버샘플링 기법을 병행하면서, `main.py`의 손실 함수 부분에 반드시 이 클래스 가중치 결합 로직을 구현하는 것이 최우선 권장 사항이다.

### 2. 일상 소음(Background Noise) 극복을 위한 비지도 도메인 적응(UDA) 및 동적 데이터 증강

앞서 데이터 분석 장에서 밝혔듯, 현존하는 최상급 오픈 데이터인 ScienceDB의 PSG-Audio 데이터는 외부 소음이 철저히 차단되고 통제된 병원 임상 수면 검사실(In-lab environment)에서 수집된 매우 '깨끗한(Clean)' 이상적 도메인의 데이터이다. 그러나 개발자가 런칭하고자 하는 최종 시스템이 스마트폰 앱의 형태가 되어 사용자의 일상 침실이나 일반적인 가정 환경(Home environment)에서 동작하게 된다면 상황은 180도 달라진다. 모델은 수면 호흡음 외에도 선풍기나 에어컨의 기계식 모터 소음, 반려동물의 짖음, 같은 침대를 쓰는 동침자의 코골이나 뒤척임, 그리고 창문 너머의 외부 교통 소음 등 극도로 동적이고 전혀 제어할 수 없는 막대한 배경 소음(Background Noise) 폭격에 고스란히 노출된다. 병원의 청정 환경 데이터(Source Domain)로만 곱게 훈련된 딥러닝 모델을 이런 거친 일상 환경(Target Domain)에 그대로 배포할 경우, 모델이 인식하는 스펙트로그램 특징 공간이 붕괴되어 수면 단계 예측 정확도가 무작위 추측(Random guess) 수준으로 급감하는 처참한 배포 실패 현상이 발생한다.

이러한 소스 도메인과 타겟 도메인 간의 거대한 간극(Domain Shift)을 메우기 위해 적용되어야 하는 필수적인 첨단 인공지능 전략이 바로 '비지도 도메인 적응(Unsupervised Domain Adaptation, UDA)' 메커니즘과 '일관성 훈련(Consistency Training)' 기법이다. 시스템의 모델 설계자는 입력 파이프라인 단계에서 프리사운드(Freesound API) 등 오픈 소음 라이브러리에서 추출한 수천 개의 다양한 일상 소음 클립(예: 실내 소음, TV 대화 소리, 바람 소리 등 8,255개 이상의 클립)을 병원의 깨끗한 오디오 신호 위에 동적인 신호 대 잡음비(SNR) 비율로 무작위 합성(Data Augmentation)하여 인위적인 극한의 소음 환경 데이터를 대량 생성하고 학습에 투입해야 한다.

나아가 네트워크 구조적으로는 DANN(Domain Adversarial Neural Networks) 이론을 차용하여 적대적 도메인 판별기(Domain Discriminator) 네트워크를 트랜스포머 인코더 또는 CNN 피처 추출기 직후에 병렬 분기로 부착하는 방식을 취해야 한다. 이 판별기는 추출된 오디오 특징(Feature) 벡터가 과연 조용한 '병원 환경'의 것인지 잡음이 많은 '가정 환경'의 것인지를 맞추도록 훈련되는 반면, 앞단의 특징 추출기는 판별기를 속이기 위해(적대적 학습, Adversarial learning) 도메인의 차이를 나타내는 정보(예: 배경 선풍기 소리, 방의 울림 특성)를 의도적으로 지워버리고, 오로지 인간 호흡의 음향 역학 자체의 본질적 특성(Domain-invariant features)만을 추출하도록 강제받게 된다. 이 비지도 도메인 적응 방법론은 SoundSleepNet을 비롯한 모바일 기기용 오디오 수면 추정 선행 연구에서 각성(Wake) 63.4%, 렘수면(REM) 64.9%, 비렘수면(NREM) 83.6%라는 놀라운 수준으로 실환경 추론 정확도를 방어해내는 결정적인 요인으로 입증된 바 있으므로, 시스템 상용화 시 반드시 통합해야 할 기술이다.

### 3. 단기 음향 특징과 장기 시간적 맥락(Long-term Temporal Context) 모델링의 이중 융합

수면 생리학에 따르면, 수면은 분절되고 정적인 무작위 상태의 연속이 아니라, 전야(Whole-night)에 걸쳐 거대한 생체 리듬의 주기성을 띠고 점진적으로 동적 전이되는 매끄러운 생리적 시퀀스 프로세스이다. 만약 딥러닝 모델이 전체의 맥락을 무시하고 오디오 신호를 단순히 30초 단위로 짧게 자른 단일 프레임(Single-epoch)만을 독립적으로 분석하게 되면, 수면 단계 전이(Stage transition) 구간에서 생리적 연속성 정보의 손실이 발생하여, 얕은 수면에서 갑자기 각성으로 뛰었다가 다시 깊은 수면으로 전락하는 식의 비상식적인 예측 플리커링(Flickering, 연속되지 않고 튀는 예측 값) 오류가 수면 그래프(Hypnogram) 전반에 걸쳐 빈번하게 일어난다.

알고리즘 분석가들은 이 문제를 해결하고 임상적으로 유효한 안정적인 수면 구조를 출력하기 위해 두 가지 상호 보완적인 시간적 맥락 모델링 접근법을 신경망 전후단에 결합해야 한다.

- **미시적-거시적 특징 융합 (Micro-Macro Feature Fusion)**: 단일 30초 오디오 스펙트로그램에서 추출된 국소적인 주파수 특징(Micro-level acoustic pattern)을 CNN 기반의 YAMNET이나 AST로 먼저 추출한다. 그런 다음, 이 압축된 특징 벡터들을 독립적으로 판별하지 않고 다시 거대한 양방향 LSTM(bi-LSTM) 층이나 시퀀스-투-시퀀스(Seq2Seq) 트랜스포머 디코더의 시간축 타임스텝 단위로 연속적으로 넘겨준다. 이를 통해 네트워크가 현재 에포크뿐만 아니라 인접한 전후 에포크(최소 3~5개, 즉 90초에서 150초 길이의 이동 창)의 거시적 컨텍스트를 동시에 파악하여 예측값을 부드럽게 평활화(Smoothing)하도록 아키텍처를 강제해야 한다. 앞서 언급한 U-Sleep과 같은 의료 시계열 파이프라인은 1D U-Net 기반의 완전 합성곱 구조를 사용하여 이러한 시간적 수용 영역(Receptive field)을 아예 전야(Whole-night) 수면 시퀀스 전체로 극단적으로 확장함으로써, 이전 단계의 정보가 현재 단계 판단에 미치는 암묵적인 전이 규칙(Transition rule)과 변수 간 상호작용 의존성을 네트워크가 스스로 학습하도록 만든다.
    
- **거시적 수면 구조(Macro Sleep Stage Distribution)의 사전 확률 및 사후 보정 매핑**: 실제 사람의 수면 주기는 시간대에 따라 발현 확률이 극적으로 다르다. 초저녁 수면 초기에는 신체의 회복을 위한 깊은 서파 수면(NREM 3, Deep)이 압도적으로 우세하고, 새벽 기상 시간에 가까워질수록 꿈을 꾸는 렘수면(REM)의 지속 시간과 빈도의 비중이 급격히 높아지는 뚜렷한 시간적 비대칭성(Asymmetry) 패턴을 띤다. 따라서 모델의 정확도를 한계치까지 끌어올리기 위해서는, 입력되는 순수 오디오 음향 피처의 마지막 차원에 '수면 시작 시점으로부터 현재 에포크까지 경과된 시간(Time elapsed from the beginning of the night)'이라는 강력한 메타데이터를 0에서 1 사이의 값으로 정규화하여 네트워크의 피드포워드 계층에 명시적으로 주입해야 한다. 나아가, 단순히 30초 단위의 국소 분류 예측 결과에만 의존하지 말고, 이러한 예측값들을 밤새도록 집계하여 전체 밤의 4대 거시 수면 단계 분포 비율(Task 2: Macro Sleep-Stage Distribution Prediction)로 역산출함으로써 전체 수면 효율(Sleep Efficiency)과 생리적 수면 구조의 무결성을 최종적으로 점검하고 안정화시키는 사후 보정(Post-processing) 기법이 파이프라인의 끝단에 필수적으로 수반되어야 한다.
    

## 결론 및 실무적 파이프라인 설계 제언

전 세계적으로 급증하는 수면 호흡 장애와 수면 질환의 위험성을 고려할 때, 복잡한 전선 연결과 고비용이 요구되는 기존 PSG 검사의 한계를 극복하고 사용자 친화적인 진단을 가능하게 하는 오디오 기반 시스템의 상용화는 필연적인 의료 기술의 진보 방향이다. 방대한 양의 선행 문헌과 가용한 국가 및 병원급 데이터베이스 구조를 종합적으로 심층 분석한 결과, 오디오를 이용한 수면 단계 비율 추정 및 무호흡 질환 실시간 탐지 시스템의 성공적인 자체 벤치마킹 및 상용 개발을 위해서는 다음과 같은 체계적이고 단계적인 기술 도입 프로세스가 강하게 요구된다.

첫째, **데이터의 획득 및 전처리 파이프라인의 전략적 구축**이다. 현재 타의 추종을 불허하는 가장 높은 해상도와 신뢰성을 제공하는 ScienceDB의 `PSG-Audio` 오픈 데이터셋(DOI: 10.11922/sciencedb.00345) 원본 확보를 최우선으로 하여 알고리즘 설계의 베이스라인(Baseline)으로 삼아야 한다. 초기 기획 및 모델의 빠른 디버깅 단계에서 막대한 연산 부하를 최소화하기 위해, Kaggle 플랫폼 등에 공유된 16kHz 다운샘플링 버전의 NPY 포맷 변환본을 적극 병행 활용하여 모델 입력 차원을 경량화하고 최적화의 속도를 높여야 한다. 특히, 접촉식 마이크(Tracheal microphone) 신호는 호흡의 순수 기계적 진동과 폐쇄성 이벤트를 규명하는 엄격한 라벨링 도우미로 사용하고, 실제 시스템의 최종 모바일 기기 추론 배포를 철저히 가정하여 공간 마이크(Ambient Microphone) 채널을 주력 학습 피처로 삼는 이원화 훈련 전략이 필요하다.

둘째, **최첨단 오픈 아키텍처의 선별적 차용 및 하이브리드 변형**이다. 무에서 유를 창조하듯 밑바닥부터 새로운 심층 신경망을 설계하여 훈련 시간과 비용을 낭비하는 대신, 오디오 특성 추출 영역에서 이미 수많은 논문을 통해 압도적인 신뢰성이 입증된 오픈소스 레퍼런스 저장소들을 적극적으로 포크(Fork)하여 목표 태스크에 맞게 파인튜닝(Fine-tuning)하는 방식이 훨씬 합리적이고 안전한 접근이다. 단순한 수면 단계 추정이나 코골이 유무 파악 수준에 머무르려 한다면, GitHub 커뮤니티에 공개된 모바일 친화적인 `Sleep-Stage-Classification` 구현체(AST 및 YAMNET 활용 기반)나 다변량 시계열 병렬 처리에 특화된 `AttnSleep` 모델의 소스코드를 클론하여 기본 골격으로 삼아야 한다. 그러나 만일 정확한 무호흡-저호흡 지수(AHI)를 정밀 산출하는 의료 기기 수준의 고도화가 목표라면, `ApneaWhisper` 논문에서 입증된 파괴적인 접근법을 따라야 한다. 즉, 입력의 특징 인출기(Feature Extractor) 부분을 수만 시간 학습된 거대 음성 파운데이션 모델(Whisper Encoder 또는 HuBERT)로 전면 치환하여 10ms 단위의 미세한 수면 호흡 프레임 변화를 포착하게 하고, 이를 수면 역학에 맞게 재단된 시계열 디코더와 결합하는 진보된 하이브리드 아키텍처를 자체적으로 구현하는 방향을 강력히 추천한다.

셋째, **실세계 환경 무중단 배포를 위한 알고리즘의 선제적 강건성(Robustness) 확보**이다. 통제된 수면 검사실에서 획득한 깨끗한 PSG-Audio 데이터에만 의존해 신경망 훈련을 마무리하게 되면, 모델은 특정 마이크의 주파수 응답이나 조용한 배경 환경 자체에 과적합(Overfitting)되어 실제 사용자의 시끄러운 침실 환경(Home environment) 서비스화 시 참담한 추론 실패를 겪게 된다. 따라서 학습 프로세스 전반에 걸쳐 Freesound와 같은 오픈 데이터베이스의 일상 소음을 무작위로 합성하는 공격적인 데이터 증강(Data Augmentation) 및 도메인 적응(Domain Adaptation) 파이프라인을 구축하여 모델의 잡음 면역력을 끌어올려야 한다. 동시에 훈련 시 비용 함수에 클래스 불균형 교차 엔트로피 손실 함수(Class-Balanced Cross-Entropy Loss)를 선제적으로 적용하여, 전체 수면 중 발현 빈도가 낮아 무시되기 쉬운 비정형 이벤트(REM 수면, 희귀 중추성 무호흡 등)에 대한 예측 민감도를 끝까지 보존해야 한다. 또한 입력 피처에 밤의 진행을 알리는 시간적 경과 정보(Time elapsed)를 반드시 스칼라 값으로 결합하여, 인간 고유의 거시적인 수면 주기 역학과 시간적 비대칭성을 시스템이 자연스럽게 통계적으로 내재화하도록 설계해야만 한다.

종합하자면, 인간의 기도와 흉곽을 거쳐 발산되는 오디오 신호는 불편하고 육중한 전통적인 PSG의 다중 접촉 생리 센서들을 일정 부분 충분히 대체할 수 있는, 강력한 잠재력이 임상적으로 완벽히 입증된 차세대 디지털 바이오마커이다. 풍부한 전문가 수동 라벨링을 제공하는 ScienceDB의 `PSG-Audio` 오픈 데이터셋을 중심 자원으로 삼고, Whisper, HuBERT, AST 기반의 최신 딥러닝 트랜스포머 기술론을 유기적으로 융합하며, 동시에 논의된 소음 도메인 전이 및 극단적 시계열 불균형 한계점들을 시스템 아키텍처 설계 단계에서부터 방어적으로 철저히 극복해 낸다면, 고가의 병원 장비 수준에 준하는 혁신적이고 고정밀의 무구속(Non-contact) 수면 단계 추정 및 진단 보조 시스템을 충분히 상용화 수준으로 구현해 낼 수 있을 것이다.
